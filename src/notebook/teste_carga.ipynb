{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99143669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, last, month, year, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para a camada Bronze\n",
    "BRONZE_PATH = '/home/Luciano/Documents/Projetos/Rep/projeto-indicadores-bcb/data/bronze'\n",
    "SILVER_PATH = '/home/Luciano/Documents/Projetos/Rep/projeto-indicadores-bcb/data/silver'\n",
    "# Define o período de anos para a carga\n",
    "ANO_INICIO_CARGA = 2015\n",
    "ANO_FIM_CARGA = datetime.now().year\n",
    "\n",
    "# Dicionário com as informações dos indicadores\n",
    "INDICADORES = {\n",
    "    'selic': {\n",
    "        'id_serie': '11',\n",
    "        'prefixo_arquivo': 'selic_', # Prefixo para o nome do arquivo\n",
    "        'nome_tabela_pg': 'taxa_selic_mensal'\n",
    "    },\n",
    "    'ipca': {\n",
    "        'id_serie': '433',\n",
    "        'prefixo_arquivo': 'ipca_', # Prefixo para o nome do arquivo\n",
    "        'nome_tabela_pg': 'taxa_ipca_mensal'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configurações definidas. Período de carga: de {ANO_INICIO_CARGA} a {ANO_FIM_CARGA}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7228cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executar_carga_completa_bronze(indicador_nome, indicador_info):\n",
    "    \"\"\"\n",
    "    Para um determinado indicador, itera de ANO_INICIO_CARGA até o ano atual.\n",
    "    Verifica se o arquivo daquele ano existe na camada Bronze.\n",
    "    Se não existir, executa uma carga para o ano completo.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processando indicador: {indicador_nome.upper()} ---\")\n",
    "\n",
    "    # Loop para cada ano no intervalo definido\n",
    "    for ano in range(ANO_INICIO_CARGA, ANO_FIM_CARGA + 1):\n",
    "        print(f\"  Verificando ano: {ano}\")\n",
    "\n",
    "        # 1. CONSTRUIR O NOME E O CAMINHO DO ARQUIVO ANUAL\n",
    "        nome_arquivo = f\"{indicador_info['prefixo_arquivo']}{ano}.json\"\n",
    "        caminho_arquivo_anual = os.path.join(BRONZE_PATH, nome_arquivo)\n",
    "\n",
    "        # 2. VERIFICAR SE O ARQUIVO DO ANO ESPECÍFICO JÁ EXISTE\n",
    "        if os.path.exists(caminho_arquivo_anual):\n",
    "            print(f\"    Arquivo '{nome_arquivo}' já existe. Pulando.\")\n",
    "            continue # Pula para o próximo ano\n",
    "\n",
    "        # 3. SE NÃO EXISTIR, FAZER A CARGA PARA O ANO\n",
    "        print(f\"    Arquivo não encontrado. Iniciando carga para o ano de {ano}.\")\n",
    "        \n",
    "        # Define o período de 1 de janeiro a 31 de dezembro do ano corrente\n",
    "        data_inicio_ano = f'01/01/{ano}'\n",
    "        data_fim_ano = f'31/12/{ano}'\n",
    "\n",
    "        url = (\n",
    "            f\"https://api.bcb.gov.br/dados/serie/bcdata.sgs.{indicador_info['id_serie']}/dados?\"\n",
    "            f\"formato=json&dataInicial={data_inicio_ano}&dataFinal={data_fim_ano}\"\n",
    "        )\n",
    "        \n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            dados = response.json()\n",
    "            if not dados: # Verifica se a API retornou uma lista vazia\n",
    "                print(f\"    A API não retornou dados para o ano de {ano}. Nenhum arquivo será criado.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"    Sucesso! {len(dados)} registros encontrados para {ano}.\")\n",
    "\n",
    "            # 4. SALVAR OS DADOS NO ARQUIVO ANUAL\n",
    "            os.makedirs(BRONZE_PATH, exist_ok=True)\n",
    "            with open(caminho_arquivo_anual, 'w', encoding='utf-8') as f:\n",
    "                json.dump(dados, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            print(f\"    Dados de {ano} salvos com sucesso em '{nome_arquivo}'\")\n",
    "        else:\n",
    "            print(f\"    Erro ao buscar dados para o ano de {ano}. Status Code: {response.status_code}\")\n",
    "\n",
    "    print(f\"--- Fim do processamento para: {indicador_nome.upper()} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef1afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nome, info in INDICADORES.items():\n",
    "    executar_carga_completa_bronze(nome, info)\n",
    "\n",
    "print(\"\\nProcesso de carga inicial concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1828158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CAMADA SILVER (Nova função)\n",
    "# =============================================================================\n",
    "def transformar_bronze_para_silver_parquet(spark, indicador_nome, indicador_info):\n",
    "    \"\"\"\n",
    "    Função 1: Lê dados JSON da camada Bronze, aplica transformações\n",
    "    e salva o resultado em formato Parquet na camada Silver.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- [SILVER-TRANSFORM] Processando indicador: {indicador_nome.upper()} ---\")\n",
    "    \n",
    "    # Encontra os arquivos JSON na camada Bronze\n",
    "    caminho_padrao_bronze = os.path.join(BRONZE_PATH, f\"{indicador_info['prefixo_arquivo']}*.json\")\n",
    "    lista_de_arquivos = glob.glob(caminho_padrao_bronze)\n",
    "    \n",
    "    if not lista_de_arquivos:\n",
    "        print(f\"  Nenhum arquivo encontrado para o padrão '{caminho_padrao_bronze}'. Pulando transformação.\")\n",
    "        return False # Retorna False para indicar que não há nada a carregar\n",
    "\n",
    "    print(f\"  Lendo {len(lista_de_arquivos)} arquivos da camada Bronze.\")\n",
    "    \n",
    "    # Define o schema para garantir a leitura correta dos dados\n",
    "    schema_esperado = StructType([\n",
    "        StructField(\"data\", StringType(), True),\n",
    "        StructField(\"valor\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # Lê os dados usando o schema definido\n",
    "    df_bronze = spark.read.schema(schema_esperado).json(lista_de_arquivos)\n",
    "\n",
    "    # Transformações comuns\n",
    "    df_transformado = df_bronze.withColumn(\"data\", to_date(col(\"data\"), \"dd/MM/yyyy\")) \\\n",
    "                               .withColumn(\"valor\", col(\"valor\").cast(DoubleType()))\n",
    "\n",
    "    # Transformação específica para cada indicador\n",
    "    if indicador_nome == 'selic':\n",
    "        print(\"  Aplicando agregação mensal para SELIC.\")\n",
    "        df_final = df_transformado.withColumn(\"ano\", year(col(\"data\"))) \\\n",
    "                                  .withColumn(\"mes\", month(col(\"data\"))) \\\n",
    "                                  .groupBy(\"ano\", \"mes\") \\\n",
    "                                  .agg(last(\"valor\").alias(\"valor_mensal\")) \\\n",
    "                                  .orderBy(\"ano\", \"mes\")\n",
    "    elif indicador_nome == 'ipca':\n",
    "        print(\"  Renomeando colunas para IPCA.\")\n",
    "        df_final = df_transformado.withColumnRenamed(\"valor\", \"percentual_variacao\")\n",
    "    \n",
    "    print(\"  Schema final transformado:\")\n",
    "    df_final.printSchema()\n",
    "    \n",
    "    # Salva o resultado em formato Parquet na camada Silver\n",
    "    caminho_escrita_silver = os.path.join(SILVER_PATH, indicador_nome)\n",
    "    print(f\"  Gravando dados em Parquet em: {caminho_escrita_silver}\")\n",
    "    df_final.write.mode(\"overwrite\").parquet(caminho_escrita_silver)\n",
    "    \n",
    "    return True # Retorna True para indicar que a próxima etapa pode ser executada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_silver_para_postgres(spark, indicador_nome, indicador_info):\n",
    "    \"\"\"\n",
    "    Função 2: Lê os dados em formato Parquet da camada Silver\n",
    "    e os carrega para uma tabela no banco de dados PostgreSQL.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- [SILVER-LOAD] Carregando {indicador_nome.upper()} para o PostgreSQL ---\")\n",
    "    \n",
    "    caminho_leitura_silver = os.path.join(SILVER_PATH, indicador_nome)\n",
    "    \n",
    "    # Lê os dados em Parquet da camada Silver\n",
    "    print(f\"  Lendo dados Parquet de: {caminho_leitura_silver}\")\n",
    "    df_silver = spark.read.parquet(caminho_leitura_silver)\n",
    "\n",
    "    # Configurações de conexão com o banco de dados\n",
    "    pg_properties = {\"user\": \"user_silver\", \"password\": \"password_silver\", \"driver\": \"org.postgresql.Driver\"}\n",
    "    pg_url = \"jdbc:postgresql://localhost:5433/silver_db\"\n",
    "\n",
    "    # Escreve o DataFrame na tabela do PostgreSQL\n",
    "    print(f\"  Gravando dados na tabela '{indicador_info['nome_tabela_pg']}'\")\n",
    "    df_silver.write.jdbc(\n",
    "        url=pg_url,\n",
    "        table=indicador_info['nome_tabela_pg'],\n",
    "        mode=\"overwrite\",\n",
    "        properties=pg_properties\n",
    "    )\n",
    "    print(\"  Dados carregados no PostgreSQL com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUÇÃO PRINCIPAL\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Executa a camada Bronze\n",
    "    for nome, info in INDICADORES.items():\n",
    "        executar_carga_completa_bronze(nome, info)\n",
    "    print(\"\\n✅ Processo da camada Bronze concluído!\")\n",
    "\n",
    "    # 2. Inicializa a Sessão Spark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PipelineIndicadoresEconomicos\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    print(\"\\n>>> Sessão Spark criada com sucesso! <<<\\n\")\n",
    "\n",
    "    # 3. Executa a camada Silver em duas etapas\n",
    "    for nome, info in INDICADORES.items():\n",
    "        # Etapa 1: Transformação de Bronze para Silver (Parquet)\n",
    "        sucesso_transformacao = transformar_bronze_para_silver_parquet(spark, nome, info)\n",
    "        \n",
    "        # Etapa 2: Carregamento de Silver (Parquet) para o PostgreSQL\n",
    "        # Só executa se a transformação gerou arquivos\n",
    "        if sucesso_transformacao:\n",
    "            carregar_silver_para_postgres(spark, nome, info)\n",
    "\n",
    "    # 4. Para a Sessão Spark\n",
    "    spark.stop()\n",
    "    print(\"\\n✅ Processo da camada Silver concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PipelineIndicadoresEconomicos\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Sessao spark criada\")\n",
    "\n",
    "print(f\"\\n--- [SILVER-TRANSFORM] Processando indicador: {INDICADORES['selic']} ---\")\n",
    "for nome, info in INDICADORES.items():\n",
    "\n",
    "    print(nome)\n",
    "    print(info)\n",
    "\n",
    "nome_indicador = 'ipca'\n",
    "print(nome_indicador)\n",
    "caminho_padrao_bronze = os.path.join(BRONZE_PATH, f\"{INDICADORES['ipca']['prefixo_arquivo']}*.json\")\n",
    "print(caminho_padrao_bronze)\n",
    "lista_de_arquivos = glob.glob(caminho_padrao_bronze)\n",
    "print(lista_de_arquivos)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"  Lendo {len(lista_de_arquivos)} arquivos da camada Bronze.\")\n",
    "\n",
    "schema_esperado = StructType([\n",
    "    StructField(\"data\", StringType(), True),\n",
    "    StructField(\"valor\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_bronze = spark.read.option(\"multiline\", \"true\").schema(schema_esperado).json(lista_de_arquivos)\n",
    "print(df_bronze)\n",
    "df_bronze.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformado = df_bronze.withColumn(\"data\", to_date(col(\"data\"), \"dd/MM/yyyy\")) \\\n",
    "                               .withColumn(\"valor\", col(\"valor\").cast(DoubleType()))\n",
    "print(df_transformado)\n",
    "df_transformado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54daa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Transformação específica para cada indicador\n",
    "if nome_indicador == 'selic':\n",
    "    print(\"  Aplicando agregação mensal para SELIC.\")\n",
    "    df_transformado = df_transformado.withColumn(\"ano\", year(col(\"data\"))) \\\n",
    "                                .withColumn(\"mes\", month(col(\"data\"))) \\\n",
    "                                .groupBy(\"ano\", \"mes\") \\\n",
    "                                .agg(last(\"valor\").alias(\"valor_mensal\")) \\\n",
    "                                .orderBy(\"ano\", \"mes\")\n",
    "    df_final = df_transformado.withColumn(\"indicador\", lit(\"SELIC\"))\n",
    "elif nome_indicador == 'ipca':\n",
    "    print(\"  Renomeando colunas para IPCA.\")\n",
    "    df_transformado = df_transformado.withColumnRenamed(\"valor\", \"percentual_variacao\")\n",
    "    df_final = df_transformado.withColumn(\"indicador\", lit(\"IPCA\"))\n",
    "\n",
    "print(df_final)\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f201d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_escrita_silver = os.path.join(SILVER_PATH, nome_indicador)\n",
    "print(caminho_escrita_silver)\n",
    "print(f\"  Gravando dados em Parquet em: {caminho_escrita_silver}\")\n",
    "df_final.write.mode(\"overwrite\").parquet(caminho_escrita_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_parquet_selic = '/home/Luciano/Documents/Projetos/Rep/projeto-indicadores-bcb/data/silver/selic'\n",
    "caminho_parquet_ipca = '/home/Luciano/Documents/Projetos/Rep/projeto-indicadores-bcb/data/silver/ipca'\n",
    "\n",
    "#df_selic_silver = spark.read.parquet(caminho_parquet_selic)\n",
    "#df_selic_silver.printSchema()\n",
    "#df_selic_silver.show()\n",
    "\n",
    "df_ipca_silver = spark.read.parquet(caminho_parquet_ipca)\n",
    "df_ipca_silver.printSchema()\n",
    "df_ipca_silver.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d4e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- [SILVER-LOAD] Carregando {INDICADORES['selic']} para o PostgreSQL ---\")\n",
    "\n",
    "nome_indicador = 'ipca'\n",
    "print(nome_indicador)\n",
    "      \n",
    "caminho_padrao_silver = os.path.join(SILVER_PATH, nome_indicador)\n",
    "print(caminho_padrao_silver)\n",
    "\n",
    "# Lê os dados em Parquet da camada Silver\n",
    "print(f\"  Lendo dados Parquet de: {caminho_padrao_silver}\")\n",
    "df_silver = spark.read.parquet(caminho_padrao_silver)\n",
    "df_silver.printSchema\n",
    "df_silver.show()                               \n",
    "print(\"-------------------\")\n",
    "\n",
    "# Configurações de conexão com o banco de dados\n",
    "pg_properties = {\"user\": \"user_silver\", \"password\": \"password_silver\", \"driver\": \"org.postgresql.Driver\"}\n",
    "pg_url = \"jdbc:postgresql://localhost:5433/silver_db\"\n",
    "\n",
    "print(INDICADORES['ipca'])\n",
    "for nome, info in INDICADORES.items():\n",
    "    print(info)\n",
    "# Escreve o DataFrame na tabela do PostgreSQL\n",
    "print(f\"  Gravando dados na tabela '{INDICADORES['ipca']['nome_tabela_pg']}'\")\n",
    "df_silver.write.jdbc(\n",
    "    url=pg_url,\n",
    "    table=INDICADORES['ipca']['nome_tabela_pg'],\n",
    "    mode=\"overwrite\",\n",
    "    properties=pg_properties\n",
    ")\n",
    "print(\"  Dados carregados no PostgreSQL com sucesso.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
